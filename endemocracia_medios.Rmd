---
title: "Monitoreo de Medios"
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: fill
    theme: lumen
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(flexdashboard)
```

```{r}
rm(list=ls())
library(tm)
library(wordcloud2)
library(readxl)
library(dplyr)
library(stringr)
library(lubridate)
library(rtweet)
library(DT)
nube<-function(aux,palabras){
  docs<-Corpus(VectorSource(aux))
  docs <- docs %>%
    tm_map(removeNumbers) %>%
    tm_map(removePunctuation) %>%
    tm_map(stripWhitespace)
  docs <- tm_map(docs, content_transformer(tolower))
  docs <- tm_map(docs, removeWords, stopwords("sp"))
  docs <- tm_map(docs, removeWords, palabras)
  dtm <- TermDocumentMatrix(docs) 
  matrix <- as.matrix(dtm) 
  words <- sort(rowSums(matrix),decreasing=TRUE) 
  df <- data.frame(word = names(words),freq=words)    
  return(df)
}
cleanPosts <- function(text) {
  clean_texts <- text %>%
    gsub("<.*>", "", .) %>% # remove emojis
    gsub("&amp;", "", .) %>% # remove &
    gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", .) %>% # remove retweet entities
    gsub("@\\w+", "", .) %>% # remove at people
    gsub("[[:punct:]]", "", .) %>% # remove punctuation
    gsub("[[:digit:]]", "", .) %>% # remove digits
    gsub("http\\w+", "", .) %>% # remove html links
    gsub("[ \t]{2,}", " ", .) %>% # remove unnecessary spaces
    gsub("^\\s+|\\s+$", "", .) %>% # remove unnecessary spaces
    tolower
  return(clean_texts)
}
load(url("http://200.105.173.107:8080/endemocracia/medios.RData"))
tw_hoy<-bd_medios %>% filter(as_date(created_at)==today())
tw_pon<-tw_hoy %>% mutate(nn=1) %>% group_by(screen_name) %>% mutate(nn=cumsum(nn)) %>% filter(nn<=10)
```


# Tendencia en redes ``r today()``

Row
-----------------------------------------------------------------------

### Número de medios

```{r}
valueBox(length(unique(tw_hoy$user_id)), icon = "fa-pencil")
```

### Publicaciones analizadas

```{r}
valueBox(nrow(tw_hoy), icon = "fa-comments")
```

### Publicaciones ponderadas

```{r}
valueBox(nrow(tw_pon), icon = "fa-trash")
```

Row
-----------------------------------------------------------------------

### Publicaciones globales

```{r}
df1<-nube(tw_hoy$text,c("bolivia",tw_medios$screen_name,"btvinforma"))
w1<-wordcloud2(data=df1,color='random-dark',size=0.5,shape = 'pentagon')
w1
```

### Publicaciones Ponderadas

```{r}
#controlando el peso de máximo 10 noticias por medio
df2<-nube(tw_pon %>%  select(text),c("bolivia",tw_medios$screen_name))
w2<-wordcloud2(data=df2,color='random-dark',size=0.5,shape = 'pentagon')
w2
```


# Análisis de Redes

```{r}
library(tm)
library(igraph)
library(visNetwork) 
# Build corpus
corpus<-tw_pon$text

corpus <- Corpus(VectorSource(corpus))

aux_drop<-c("aquí","envivo","lea","día","nel","nla","𝐩𝐚𝐫𝐚","𝐥𝐚","𝐜𝐨𝐧","👉","𝐚","𝐞𝐥","piedrapapelytinta","streaminglarazón","𝐲","🔴","<U+0001F534>")

# Clean text
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
cleanset <- tm_map(corpus, removeWords, c(stopwords('spanish'),"𝐝𝐞","nota"))
cleanset <- tm_map(cleanset, removeWords, aux_drop)
removeURL <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
cleanset <- tm_map(cleanset, stripWhitespace)

# Term document matrix
tdm <- TermDocumentMatrix(cleanset)
tdm <- as.matrix(tdm)
tdm <- tdm[rowSums(tdm)>=6,]

# Network of terms
tdm[tdm>1] <- 1
termM <- tdm %*% t(tdm)
g <- graph.adjacency(termM, weighted = T, mode = 'undirected')
g <- simplify(g)
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)

data <- toVisNetworkData(g)
nodes = data$nodes; edges = data$edges
grupos <- cluster_label_prop(g)

nodes$value<-prop.table(nodes$degree)*1000
nodes$group<-grupos$membership

visNetwork(nodes,edges) %>% visEdges(arrows = "to")
```


# Lista de publicaciones

```{r}
DT::datatable(tw_hoy[,c(3,4,5)],style = "bootstrap")
```

# Lista de medios

```{r}
DT::datatable(tw_medios[,c(2,3)],style = "bootstrap")
```
